{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-scraping Playground\n",
    "\n",
    "According to Wikipedia:\n",
    "> Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
    "\n",
    "We will use the website from https://genius.com/ to scrape your own favorite songs.\n",
    "\n",
    "This notebook will guide you how to scrape the lyrics and I will explain each blocks of codes step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. requests: http requests (we mainly use GET method from this library)\n",
    "2. bs4: BeautifulSoup library which is widely used in web-scraping\n",
    "3. re: for regex purpsoes\n",
    "4. tqdm: a nice progress bar for scraping indicator\n",
    "\n",
    "Next, find your favorite songs or artists! I will use the songs from a K-pop girlgroup, IZ*ONE https://genius.com/artists/Izone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify lyric's urls\n",
    "URL = ['https://genius.com/albums/Izone/Color-iz',\n",
    "       'https://genius.com/albums/Izone/Heart-iz',\n",
    "       'https://genius.com/albums/Izone/Vampire',\n",
    "       'https://genius.com/albums/Izone/Bloom-iz',\n",
    "       'https://genius.com/albums/Izone/Oneiric-diary'] # let's start with these urls\n",
    "\n",
    "TIMEOUT = 20 # set timeout before each request\n",
    "LYRIC_PATH = '../data/lyrics.txt' # path to store the scraped lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, onto the exciting part, scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in URL:\n",
    "    print('Waiting for', TIMEOUT, 'seconds before the next album...')\n",
    "    time.sleep(TIMEOUT)\n",
    "    print('Current album:', u)\n",
    "    \n",
    "    req = requests.get(u) # initiate GET request to the url\n",
    "    soup = BeautifulSoup(req.text, 'lxml') # use BeautifulSoup to find the html tags\n",
    "    \n",
    "    raw_url = []\n",
    "    for s in soup.find_all('div'):\n",
    "        link = s.find('a')\n",
    "        if link is not None:\n",
    "            raw_url.append(link.attrs['href'])\n",
    "            \n",
    "    songs = []\n",
    "    for r in raw_url:\n",
    "        x = re.search('^(http|https)://.*Izone-.*lyrics$', r)\n",
    "        if x is not None:\n",
    "            songs.append(r)\n",
    "            \n",
    "    non_duplicate_songs = []\n",
    "    for s in songs:\n",
    "        if s not in non_duplicate_songs:\n",
    "            non_duplicate_songs.append(s)\n",
    "            \n",
    "    for n in non_duplicate_songs:\n",
    "        print('Waiting for', TIMEOUT, 'seconds before writing the next song...')\n",
    "        time.sleep(TIMEOUT)\n",
    "        \n",
    "        print('Writing lyrics:', n)\n",
    "        req = requests.get(n)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        lyrics = []\n",
    "        for a in tqdm(soup.find_all('div')):\n",
    "            lyric = a.find('p')\n",
    "            if lyric is not None:\n",
    "                lyrics.append(lyric)\n",
    "    \n",
    "        lyrics = clean_html_tags(lyrics[0])\n",
    "        with open(LYRIC_PATH, 'a', encoding='utf-8') as f:\n",
    "            f.write(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - GET Request   \n",
    "\n",
    "    req = requests.get(u) # initiate GET request to the url\n",
    "    soup = BeautifulSoup(req.text, 'lxml') # use BeautifulSoup to find the html tags\n",
    "\n",
    "There are several HTTP Requests. In this project, we only use the GET request to retrieve the body of the HTML. Then, BeautifulSoup will parse them into plain text with lxml format.\n",
    "\n",
    "Be careful when you parse non-English lyrics. When you don't specify the parsing format, it will scrape weird looking characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Scraping Links\n",
    "\n",
    "    raw_url = []\n",
    "    for s in soup.find_all('div'):\n",
    "        link = s.find('a')\n",
    "        if link is not None:\n",
    "            raw_url.append(link.attrs['href'])\n",
    "    \n",
    "Before scraping, it is a good practice to look inside the website yourself and inspect them. Press F12 or right click then inspect element to view the HTML.\n",
    "Genius always structures their body like this\n",
    "\n",
    "![Genius HTML Body](../assets/html_body_1.png)\n",
    "\n",
    "Notice that each of the songs in the album are arranged in order:\n",
    "```html\n",
    "<div>\n",
    "    content goes here...\n",
    "    <div>\n",
    "        content goes here...\n",
    "        <a href> link to song 1 </a>\n",
    "    </div>\n",
    "    <div>\n",
    "        content goes here...\n",
    "        <a href> link to song 2 </a>\n",
    "    </div>\n",
    "    <div>\n",
    "        content goes here...\n",
    "        <a href> link to song 3 </a>\n",
    "    </div>\n",
    "    ...\n",
    "</div>\n",
    "```\n",
    "\n",
    "This structure makes it easier for us to scrape out of the link to the songs in an album. We can use find_all('div') to retrieve all the content inside the 'div' tag, then use find('a') to retrieve those links.\n",
    "\n",
    "    songs = []\n",
    "    for r in raw_url:\n",
    "        x = re.search('^(http|https)://.*Izone-.*lyrics$', r)\n",
    "        if x is not None:\n",
    "            songs.append(r)\n",
    "            \n",
    "    non_duplicate_songs = []\n",
    "    for s in songs:\n",
    "        if s not in non_duplicate_songs:\n",
    "            non_duplicate_songs.append(s)\n",
    "            \n",
    "This is where re library is useful. You usually find unrelated contents (links) which are not the part of the album. Use re.search(pattern) to find the related links. You can sometimes encounter duplicate links when scraping. Add another line of codes to copy non-duplicate links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Scraping Lyrics\n",
    "\n",
    "    for n in non_duplicate_songs:\n",
    "        print('Waiting for', TIMEOUT, 'seconds before writing the next song...')\n",
    "        time.sleep(TIMEOUT)\n",
    "        \n",
    "        print('Writing lyrics:', n)\n",
    "        req = requests.get(n)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        lyrics = []\n",
    "        for a in tqdm(soup.find_all('div')):\n",
    "            lyric = a.find('p')\n",
    "            if lyric is not None:\n",
    "                lyrics.append(lyric)\n",
    "    \n",
    "        lyrics = clean_html_tags(lyrics[0])\n",
    "        with open(LYRIC_PATH, 'a', encoding='utf-8') as f:\n",
    "            f.write(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
