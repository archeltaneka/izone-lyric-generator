{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics text file\n",
    "with open('./data/lyrics.txt', encoding='utf-8') as f:\n",
    "    raw_lyrics = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 1792\n",
      "Total number of unique words (roughly): 1964\n",
      "Average number of words in a line: 3.763392857142857\n",
      "The least number of words in a line: 0\n",
      "The most number of words in a line: 13\n",
      "\n",
      "Lyric preview:\n",
      "Have you ever seen anything?\n",
      "아름다운 색, 아름다운 색, 아름다운 색\n",
      "Have you ever seen this color?\n",
      "아름다운 색, 아름다운 다운 다운 다운\n",
      "Have you ever seen anything?\n",
      "아름다운 색, 아름다운 색, 아름다운 색\n",
      "Have you ever seen this color?\n",
      "아름다운 색, 아름다운 다운 다운 다운\n",
      "\n",
      "끌리네 그 누구와도 다르게\n",
      "변하고 싶어 나\n",
      "너를 바라보면서 yeah\n",
      "너를 알아가면서 yeah\n",
      "\n",
      "상상이 내 감정을 더 움직여\n",
      "열두 가지 색색깔의 무지개\n",
      "나는 과연 어떤 색일까\n",
      "우리 더 빛나게 해볼까\n",
      "\n",
      "천천히 하나 둘 그리는 하얀 종이 위에\n"
     ]
    }
   ],
   "source": [
    "# data statistics\n",
    "lyrics_per_line = raw_lyrics.split('\\n')\n",
    "word_count_line = [len(line.split()) for line in lyrics_per_line]\n",
    "\n",
    "print('Total number of lines:', len(raw_lyrics.split('\\n')))\n",
    "print('Total number of unique words (roughly):', len({word: None for word in raw_lyrics.split()}))\n",
    "print('Average number of words in a line:', np.average(word_count_line))\n",
    "print('The least number of words in a line:', np.min(word_count_line))\n",
    "print('The most number of words in a line:', np.max(word_count_line))\n",
    "print()\n",
    "\n",
    "view_range = 20\n",
    "print('Lyric preview:')\n",
    "print('\\n'.join(raw_lyrics.split('\\n')[:view_range]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# check which punctuations do the lyrics have\n",
    "def check_punctuations(lyrics):\n",
    "    flag = False\n",
    "    punct_list = []\n",
    "    for p in punctuation:\n",
    "        if raw_lyrics.find(p) != -1:\n",
    "            flag = True\n",
    "            punct_list.append(p)\n",
    "\n",
    "    return (flag, punct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, ['!', \"'\", '(', ')', ',', '-', '/', '?'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_punctuations(raw_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(lyrics):\n",
    "    word_count = Counter(lyrics)\n",
    "    sorted_word_count = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(sorted_word_count)}\n",
    "    int_to_vocab = {idx: word for word, idx in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def create_token_lookup():\n",
    "    punctuations = ['!', \"'\", '(', ')', ',', '-', '/', '?', '\\n', 'PADDING']\n",
    "    tokens = ['<EXCLAMATION_MARK>', '<SINGLE_QUOTATION_MARK>', '<LEFT_ROUND_BRACKET>', '<RIGHT_ROUND_BRACKET>',\n",
    "              '<COMMA>', '<HYPHEN>', '<SLASH>', '<QUESTION_MARK>', '<NEW_LINE>', '<PADDING>']\n",
    "    \n",
    "    punct_token = {}\n",
    "    for p in range(len(punctuations)):\n",
    "        punct_token[punctuations[p]] = tokens[p]\n",
    "        \n",
    "    return punct_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "token_lookup = create_token_lookup()\n",
    "for symbol, token in token_lookup.items():\n",
    "    raw_lyrics = raw_lyrics.replace(symbol, ' {} '.format(token))\n",
    "\n",
    "tokenized_lyrics = raw_lyrics.lower()\n",
    "tokenized_lyrics = tokenized_lyrics.split()\n",
    "    \n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(tokenized_lyrics)\n",
    "encoded_lyrics = [vocab_to_int[word] for word in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': '<EXCLAMATION_MARK>',\n",
       " \"'\": '<SINGLE_QUOTATION_MARK>',\n",
       " '(': '<LEFT_ROUND_BRACKET>',\n",
       " ')': '<RIGHT_ROUND_BRACKET>',\n",
       " ',': '<COMMA>',\n",
       " '-': '<HYPHEN>',\n",
       " '/': '<SLASH>',\n",
       " '?': '<QUESTION_MARK>',\n",
       " '\\n': '<NEW_LINE>',\n",
       " 'PADDING': '<PADDING>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available! Training on: GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability\n",
    "import torch\n",
    "\n",
    "gpu_availability = torch.cuda.is_available()\n",
    "\n",
    "if gpu_availability:\n",
    "    print('GPU Available! Training on:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU found! Training on CPU...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_lyric(lyrics, sequence_length, batch_size):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for w in range(len(lyrics)):\n",
    "        if w+sequence_length < len(lyrics):\n",
    "            features.append(lyrics[w:w+sequence_length])\n",
    "            labels.append(lyrics[w+sequence_length])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    dataset = TensorDataset(torch.from_numpy(features), torch.from_numpy(labels))\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  22,   22,   22,   19,    3],\n",
      "        [ 154,  102,  112,    0,    0],\n",
      "        [  32,   65,   15,  284,    0],\n",
      "        [ 177,    0,   58, 1118,  477],\n",
      "        [ 490,  190,  750,  523,   35],\n",
      "        [ 705,  706,  707,  708,    0],\n",
      "        [ 731,    7,  366,  188,    1],\n",
      "        [   1,    4,    0,  271, 1690],\n",
      "        [   0,  184,  481,   38,  256],\n",
      "        [   0,    2,   11,    3,  133]], dtype=torch.int32)\n",
      "tensor([  0,   2,  38,  56,   0, 709, 366,   4, 177, 265], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_loader = batch_lyric(encoded_lyrics, sequence_length=5, batch_size=10)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "f, l = train_iter.next()\n",
    "print(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, n_input, hidden):\n",
    "        batch = n_input.size(0)\n",
    "        \n",
    "        embed = self.embedding(n_input)\n",
    "        l, hidden = self.lstm(embed, hidden)\n",
    "        l = l.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(l)\n",
    "        out = out.view(batch, -1, self.output_size)\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        w = next(self.parameters()).data\n",
    "        \n",
    "        if gpu_availability:\n",
    "            hidden = (w.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      w.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (w.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      w.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(1730, 300)\n",
      "  (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=1730, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = Model(vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_back_propagation(model, optimizer, criterion, feat, target, hidden):\n",
    "    if gpu_availability:\n",
    "        model.cuda()\n",
    "        feat, target = feat.cuda(), target.cuda()\n",
    "        \n",
    "    h = tuple([a.data for a in hidden])\n",
    "    model.zero_grad()\n",
    "\n",
    "    out, h = model(feat, h)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size, optimizer, criterion, epochs, show_every=100):\n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print('------------ EPOCH', i+1, '------------')\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for batch, (inp, labels) in enumerate(train_loader, 1):\n",
    "            inp = inp.to(torch.int64)\n",
    "            labels = labels.to(torch.int64)\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if batch > n_batches:\n",
    "                break\n",
    "            \n",
    "            # forward and back prop\n",
    "            loss, hidden = forward_and_back_propagation(model, optimizer, criterion, inp, labels, hidden)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if batch % show_every == 0:\n",
    "                print('Loss:', np.average(losses))\n",
    "                losses = []\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = batch_lyric(encoded_lyrics, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ EPOCH 1 ------------\n",
      "Loss: 5.928218336105346\n",
      "Loss: 5.416991791725159\n",
      "Loss: 5.050977783203125\n",
      "------------ EPOCH 2 ------------\n",
      "Loss: 4.404625531668975\n",
      "Loss: 4.2583461451530455\n",
      "Loss: 4.016608800888061\n",
      "------------ EPOCH 3 ------------\n",
      "Loss: 3.4264093871428587\n",
      "Loss: 3.4092815041542055\n",
      "Loss: 3.1957096552848814\n",
      "------------ EPOCH 4 ------------\n",
      "Loss: 2.5165958805619\n",
      "Loss: 2.4853733658790587\n",
      "Loss: 2.515227861404419\n",
      "------------ EPOCH 5 ------------\n",
      "Loss: 1.775161578276447\n",
      "Loss: 1.8072284328937531\n",
      "Loss: 1.7952332746982576\n"
     ]
    }
   ],
   "source": [
    "model = Model(vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "if gpu_availability:\n",
    "    model.cuda()\n",
    "    \n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = train(model, batch_size, opt, criterion, epochs, show_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_lyrics(trained_rnn, start_word, int_to_vocab, punct_token, padding, lyric_length = 100):\n",
    "    trained_rnn.eval()\n",
    "    \n",
    "    # we have only 1 word only in the beginning (start word)\n",
    "    lyric_sequence = np.full((1, sequence_length), padding)\n",
    "    lyric_sequence[-1][-1] = start_word\n",
    "    pred = [int_to_vocab[start_word]]\n",
    "    \n",
    "    for _ in range(lyric_length):\n",
    "        if gpu_availability:\n",
    "            lyric_sequence = torch.LongTensor(lyric_sequence).cuda()\n",
    "        else:\n",
    "            lyric_sequence = torch.LongTensor(lyric_sequence)\n",
    "            \n",
    "        hidden = trained_rnn.init_hidden(lyric_sequence.size(0))\n",
    "        output, _ = trained_rnn(lyric_sequence, hidden)\n",
    "        \n",
    "        candidate_lyrics = F.softmax(output, dim=1).data\n",
    "        if gpu_availability:\n",
    "            candidate_lyrics = candidate_lyrics.cpu()\n",
    "            \n",
    "        top_candidate = 5\n",
    "        candidate_lyrics, chosen_lyrics = candidate_lyrics.topk(top_candidate)\n",
    "        chosen_lyrics = chosen_lyrics.numpy().squeeze()\n",
    "        \n",
    "        candidate_lyrics = candidate_lyrics.numpy().squeeze()\n",
    "        idx = np.random.choice(chosen_lyrics, candidate_lyrics=candidate_lyrics/candidate_lyrics.sum())\n",
    "        \n",
    "        lyric = int_to_vocab[idx]\n",
    "        pred.append(lyric)\n",
    "        \n",
    "        lyric_sequence = np.roll(lyric_sequence, -1, 1)\n",
    "        lyric_sequence[-1][-1] = idx\n",
    "        \n",
    "    generated_lyrics = ' '.join(pred)\n",
    "    \n",
    "    # convert back punctuation tokens into real punctuations\n",
    "    for punct, token in token_lookup.items():\n",
    "        ending = ' ' if punct in ['\\n', '(', '\"'] else ''\n",
    "        generated_lyrics = generated_lyrics.replace(' ' + token.lower(), punct)\n",
    "    generated_lyrics = generated_lyrics.replace('\\n ', '\\n')\n",
    "    generated_lyrics = generated_lyrics.replace('( ', '(')\n",
    "    \n",
    "    return generated_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<PADDING>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-687a60666a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpad_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'<PADDING>'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgenerated_lyrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_lyrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_lyric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_lookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlyrics_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_lyrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<PADDING>'"
     ]
    }
   ],
   "source": [
    "lyrics_len = 200\n",
    "start_lyric = '하늘'\n",
    "\n",
    "pad_token = '<PADDING>'\n",
    "generated_lyrics = generate_lyrics(trained_model, vocab_to_int[start_lyric], int_to_vocab, token_lookup, vocab_to_int[pad_token], lyrics_len)\n",
    "print(generated_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
