{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics text file\n",
    "with open('./data/lyrics.txt', encoding='utf-8') as f:\n",
    "    raw_lyrics = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 1792\n",
      "Total number of unique words (roughly): 1964\n",
      "Average number of words in a line: 3.763392857142857\n",
      "The least number of words in a line: 0\n",
      "The most number of words in a line: 13\n",
      "\n",
      "Lyric preview:\n",
      "Have you ever seen anything?\n",
      "아름다운 색, 아름다운 색, 아름다운 색\n",
      "Have you ever seen this color?\n",
      "아름다운 색, 아름다운 다운 다운 다운\n",
      "Have you ever seen anything?\n",
      "아름다운 색, 아름다운 색, 아름다운 색\n",
      "Have you ever seen this color?\n",
      "아름다운 색, 아름다운 다운 다운 다운\n",
      "\n",
      "끌리네 그 누구와도 다르게\n",
      "변하고 싶어 나\n",
      "너를 바라보면서 yeah\n",
      "너를 알아가면서 yeah\n",
      "\n",
      "상상이 내 감정을 더 움직여\n",
      "열두 가지 색색깔의 무지개\n",
      "나는 과연 어떤 색일까\n",
      "우리 더 빛나게 해볼까\n",
      "\n",
      "천천히 하나 둘 그리는 하얀 종이 위에\n"
     ]
    }
   ],
   "source": [
    "# data statistics\n",
    "lyrics_per_line = raw_lyrics.split('\\n')\n",
    "word_count_line = [len(line.split()) for line in lyrics_per_line]\n",
    "\n",
    "print('Total number of lines:', len(raw_lyrics.split('\\n')))\n",
    "print('Total number of unique words (roughly):', len({word: None for word in raw_lyrics.split()}))\n",
    "print('Average number of words in a line:', np.average(word_count_line))\n",
    "print('The least number of words in a line:', np.min(word_count_line))\n",
    "print('The most number of words in a line:', np.max(word_count_line))\n",
    "print()\n",
    "\n",
    "view_range = 20\n",
    "print('Lyric preview:')\n",
    "print('\\n'.join(raw_lyrics.split('\\n')[:view_range]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# check which punctuations do the lyrics have\n",
    "def check_punctuations(lyrics):\n",
    "    flag = False\n",
    "    punct_list = []\n",
    "    for p in punctuation:\n",
    "        if raw_lyrics.find(p) != -1:\n",
    "            flag = True\n",
    "            punct_list.append(p)\n",
    "\n",
    "    return (flag, punct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, ['!', \"'\", '(', ')', ',', '-', '/', '?'])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_punctuations(raw_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(lyrics):\n",
    "    word_count = Counter(lyrics)\n",
    "    sorted_word_count = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(sorted_word_count)}\n",
    "    int_to_vocab = {idx: word for word, idx in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def create_token_lookup():\n",
    "    punctuations = ['!', \"'\", '(', ')', ',', '-', '/', '?', '\\n']\n",
    "    tokens = ['<EXCLAMATION_MARK>', '<SINGLE_QUOTATION_MARK>', '<LEFT_ROUND_BRACKET>', '<RIGHT_ROUND_BRACKET>',\n",
    "              '<COMMA>', '<HYPHEN>', '<SLASH>', '<QUESTION_MARK>', '<NEW_LINE>']\n",
    "    \n",
    "    punct_token = {}\n",
    "    for p in range(len(punctuations)):\n",
    "        punct_token[punctuations[p]] = tokens[p]\n",
    "        \n",
    "    return punct_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "token_lookup = create_token_lookup()\n",
    "for symbol, token in token_lookup.items():\n",
    "    raw_lyrics = raw_lyrics.replace(symbol, ' {} '.format(token))\n",
    "\n",
    "tokenized_lyrics = raw_lyrics.lower()\n",
    "tokenized_lyrics = tokenized_lyrics.split()\n",
    "    \n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(tokenized_lyrics)\n",
    "encoded_lyrics = [vocab_to_int[word] for word in tokenized_lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available! Training on: GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability\n",
    "import torch\n",
    "\n",
    "gpu_availability = torch.cuda.is_available()\n",
    "\n",
    "if gpu_availability:\n",
    "    print('GPU Available! Training on:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU found! Training on CPU...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_lyric(lyrics, sequence_length, batch_size):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for w in range(len(lyrics)):\n",
    "        if w+sequence_length < len(lyrics):\n",
    "            features.append(lyrics[w:w+sequence_length])\n",
    "            labels.append(lyrics[w+sequence_length])\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    dataset = TensorDataset(torch.from_numpy(features), torch.from_numpy(labels))\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,   67,   70,   46,  312],\n",
      "        [   3,    0,   34,   72,    8],\n",
      "        [ 471,    0,    0,  454,  128],\n",
      "        [ 485, 1200,    0,   47,  688],\n",
      "        [   0,    0,    6, 1137,    6],\n",
      "        [   0,    0,   38, 1391, 1392],\n",
      "        [  15,  768,   28,  523,   35],\n",
      "        [ 269, 1001,  133,    0,    0],\n",
      "        [  15,  372,  373,    2,   15],\n",
      "        [  69,   69,   69,    0,   84]], dtype=torch.int32)\n",
      "tensor([  71,    0,  101, 1201,   82,  728,    0,   90,  372,    7],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_loader = batch_lyric(encoded_lyrics, sequence_length=5, batch_size=10)\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "f, l = train_iter.next()\n",
    "print(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, n_input, hidden):\n",
    "        batch = n_input.size(0)\n",
    "        \n",
    "        embed = self.embedding(n_input)\n",
    "        l, hidden = self.lstm(embed, hidden)\n",
    "        l = l.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(l)\n",
    "        out = out.view(batch, -1, self.output_size)\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        w = next(self.parameters()).data\n",
    "        \n",
    "        if gpu_availability:\n",
    "            hidden = (w.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      w.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (w.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      w.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "            \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(1730, 300)\n",
      "  (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=1730, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = Model(vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_back_propagation(model, optimizer, criterion, feat, target, hidden):\n",
    "    if gpu_availability:\n",
    "        model.cuda()\n",
    "        feat, target = feat.cuda(), target.cuda()\n",
    "        \n",
    "    h = tuple([a.data for a in hidden])\n",
    "    model.zero_grad()\n",
    "\n",
    "    out, h = model(feat, h)\n",
    "    loss = criterion(out, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size, optimizer, criterion, epochs, show_every=100):\n",
    "    losses = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print('------------ EPOCH', i+1, '------------')\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for batch, (inp, labels) in enumerate(train_loader, 1):\n",
    "            inp = inp.to(torch.int64)\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if batch > n_batches:\n",
    "                break\n",
    "            \n",
    "            # forward and back prop\n",
    "            loss, hidden = forward_and_back_propagation(model, optimizer, criterion, inp, labels, hidden)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if batch % show_every == 0:\n",
    "                print('Loss:', np.average(losses))\n",
    "                losses = []\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = batch_lyric(encoded_lyrics, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 0.001\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ EPOCH 1 ------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Int for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-f393a723dcf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-191-b9974dd04b78>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, batch_size, optimizer, criterion, epochs, show_every)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# forward and back prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_and_back_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-190-4a49ac14f4fb>\u001b[0m in \u001b[0;36mforward_and_back_propagation\u001b[1;34m(model, optimizer, criterion, feat, target, hidden)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1838\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1839\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Int for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "model = Model(vocab_size, output_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "if gpu_availability:\n",
    "    model.cuda()\n",
    "    \n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_model = train(model, batch_size, opt, criterion, epochs, show_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
